TASK 01

Import a File into HDFS

    Create a new directory in HDFS named /user/horton/flightdelays

    Put the three files from the /home/horton/datasets/flightdelays directory on the local machine into the /user/horton/flightdelays directory in HDFS

    

     hdfs dfs -mkdir flightdelays;

  hdfs dfs -copyFromLocal  /home/horton/datasets/flightdelays/*\d*.scv /user/horton/flightdelays



TASK 02

Cleanse Data using Pig

    Notice the comma-separated values of the flightdelays files in HDFS contain historical data for airline flight delays. The columns in the files match the following schema:


    Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime,

    UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime,

    ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, Cancelled, 

    CancellationCode, Diverted, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay,

    LateAircraftDelay                

                    
    Write a Pig script that satisfies all of the following criteria:

        Load all of the data in /user/horton/flightdelays

        Remove all rows in the flightdelays data where the DepTime column equals the string "NA".

        The output should only contain the Year, Month, DayofMonth, DepTime, UniqueCarrier, FlightNum, ArrDelay, Origin and Dest

        Store the result as comma-separated records in a new directory in HDFS named /user/horton/flightdelays_clean

        Save the script in a file named flightdelays_clean.pig and save it in the /home/horton/solutions directory on the local filesystem of the client machine.


 /*
a = load 'flightdelays' using PigStorage(',');

b = filter a by (chararray) $4 != 'NA' or (chararray) $11 != 'NA';

c = foreach b generate $0 as year:int, $1 as month:int, $2 as dayofmonth:int, $4 as deptime:int, $8 as uniquecarrier:chararray, $9 as flightnum:chararray, $14 as arrdelay:int, $16 as origin:chararray, $17 as dest:chararray;

store c into '/user/horton/flightdelays_clean' using PigStorage(',');

*/

    pig_sc1 = load '/user/horton/flightdelays' using PigStorage(',') as 

    (Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime,

    UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime,

    ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, Cancelled, 

    CancellationCode, Diverted, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay,

    LateAircraftDelay );

 

 filter_sc1 = filter pig_sc1 by DepTime!='NA';

 foreach_sc1= foreach filete_sc1 generate Year, Month, DayofMonth, DepTime, UniqueCarrier, FlightNum, ArrDelay, Origin,Dest;

 store_res_sc1 = store foreach_sc1 into '/user/horton/flightdelays_clean' using PigStorage(',');

 

TASK 03

Analyze Data using Pig

    Write a Pig script saved on the client machine as /home/horton/solutions/cleaned_total.pig 

 that calculates the number of rows in the /user/horton/flightdelays_clean files in HDFS. 

 Store the output of your script in a new directory in HDFS named cleaned_total.

    The Dest column is the destination airport code where the flight ends. 

 

 Write a Pig script saved on the client machine as /home/horton/solutions/denver_total.

 pig that calculates the number of rows in the /user/horton/flightdelays_clean data where 

 the Dest field equals the Denver airport code "DEN". Store the output of your script in 

 a new directory in HDFS named denver_total.

    The ArrDelay column is the number of minutes that a flight arrived late. 

 

 Write a Pig script saved on the client machine as /home/horton/solutions/denver_late.pig that 

 counts the number of flights whose Dest is the "DEN" airport that arrived 60 or more mintues late. 

 Store the output of your script in a new directory in HDFS named denver_late

 

//Year, Month, DayofMonth, DepTime, UniqueCarrier, FlightNum, ArrDelay, Origin,Dest;

     

1)  a = load 'user/horton/flightdelays_clean' using PigStorage(',');

 b = group a all;

 c = foreach b generate count(a);

  store c into 'cleaned_total';

  desc

2)a1 = filter a by $8='DEN'

  b1 = group a1 all;

  c1 = foreach b1 generate count(a1);

  

3) a1 = filter a by $8='DEN' and $6>=60;

  b1 = group a1 all;

  c1 = foreach b1 generate count(a1);




TASK 04

Define a Hive Table

    Define a Hive table named flightdelays that matches the data stored in your /user/horton/flightdelays_clean directory in HDFS. The Hive table should satisfy all of the following criteria:

        An external table with the location set to /user/horton/flightdelays_clean

        The schema matches the columns Year, Month, DayOfMonth, DepTime, UniqueCarrier, FlightNum, ArrDelay, Origin and Dest

        The UniqueCarrier, Origin and Dest columns are string types; the other columns are all integers


 create external table flightdelays (

 year int, 

 month int, 

 dayofmonth int, 

 deptime int, 

 uniquecarrier string, 

 flightnum int, 

 arrdelay int, 

 origin string, 

 dest string) 

 row format delimited 

 fields terminated by ',' 

 location '/user/horton/flightdelays_clean';

 

TASK 05

Use HCatalog with Pig

    Write a Pig script saved on the client machine as /home/horton/solutions/flightdelays_nonzero.pig that satisfies all of the following criteria:

        Run the Pig query using Tez as the execution engine

        Load the data from the flightdelays table in Hive using HCatalog

        Remove any rows where the arrdelay is less than or equal to 0

        Order the output by the arrdelay value descending

        Store the output as three comma-separated files in a new directory in HDFS named /user/horton/flightdelays_nonzero

 pig -useHCatalog -x tez

 a = load 'flightdelays' using org.apache.hive.hcatalog.pig.HCatLoader();

 b = filter a by $6>0;

 c = order b by $6 desc parallel 3;

 store c into '/user/horton/flightdelays_nonzero' using pigStorage(',');


 
TASK 06

Analyzing Data with Hive

    Write a Hive query for each of the tasks below. Save the queries in a single text file named /home/horton/solutions/flightdelays.hive:

        Compute the average arrdelay of flights landing in Denver (dest equals "DEN")

        Compute the average arrdelay of flights where the origin is LAX and the dest is SFO

        Determine which dest airport had the highest average arrdelay

 
 select avg(arrdelay) from flightdelays where dest='DEN';

 select avg(arrdelay) from flightdelays where origin 'LAX' and dest='SFO';

 select dest,avg(arrdelay) as delay from flightdelays group by dest order by delay asc;

 
TASK 07

Define and Populate an ORCFile Table

    Define a Hive table named sfo_weather that satisfies all of the following criteria:

        A Hive-managed table

        The data is stored in the ORCFile format

        The table is populated with the records in the /home/horton/datasets/flightdelays/sfo_weather.csv file on the client machine

        The schema matches the columns in sfo_weather.csv - the first column is a string named station_name, followed by integers for the Year, Month, DayOfMonth, precipitation, temperature_max, and temperature_min


 create table sfo_weather_table(

 station_name STRING, 

 Year int, 

 Month int, 

 DayOfMonth int, 

 precipitation int, 

 temperature_max int, 

 temperature_min int

 )

 rowformat delimited

 fields terminated by ','

 stored as orc;

 

 insert table sfo_weather_table from select * from sfo_weather_temp;

 insert overwrite sfo_weather_table from select * from sfo_weather_temp;

 load data local inpath '/home/horton/datasets/flightdelays/sfo_weather.csv ' into table sfo_weather_txt;

 

 create table sfo_weather_orc stored as orc select *from sfo_weather_txt;


TASK 08

Hive Join

    Write a Hive query in a file named /home/horton/solutions/flights_weather.hive that satisfies the following criteria:

        Use Tez as the execution engine

        The result of the query is in a new Hive-managed table named flights_weather stored as a textfile

        Join the flightdelays table with the sfo_weather table where dest or origin equals "SFO" in flightdelays, and the year, month and dayofmonth are equal in the two tables

        Select all columns from the flightdelays table, and the temperature_max and temperature_min columns from sfo_weather



 create table flights_weather as

 select f.*,w.temperature_max,w.temperature_min from flightdelays f 

 join sfo_weather w on( and w.year==f.year and w.month==f.month and w.dayofmonth=f.dayofmonth) where dest='SFO' OR origin='SFO';  

 

TASK 09

Hive Partitioned Tables

    Write a Hive query in a file named /home/horton/solutions/weather_partitioned.hive that satisfies the following criteria:

        Define a new Hive-managed table named weather_partitioned that has the same schema as the sfo_weather table

        The table is partitioned on the year and month columns

        The data is stored in the ORCFile format

        
 Insert the records from January, 2008, of the sfo_weather table into the appropriate partition of weather_partitioned


 create table weather_partitioned(station_name string, dayofmonth int, precipipation int, temperature_max int, temperature_min int)

 partitioned by (year int,month int) stored as orc;

 insert into table weather_partitioned partition(month='january',year=2008) select * from sfo_weather where year=2008 and month=1;

 
TASK 10

Sqoop Export


    Put the local file /home/hortonworks/datasets/flightdelays/sfo_weather.csv into HDFS in a new directory named /user/hortonworks/weather/

    Note there is a MySQL database named flightinfo on the namenode machine. It contains a table named weather with the following schema:


    +---------------+--------------+------+-----+---------+-------+

    | Field         | Type         | Null | Key | Default | Extra |

    +---------------+--------------+------+-----+---------+-------+

    | station       | varchar(100) | YES  |     | NULL    |       |

    | year          | int(11)      | YES  |     | NULL    |       |

    | month         | int(11)      | YES  |     | NULL    |       |

    | dayofmonth    | int(11)      | YES  |     | NULL    |       |

    | precipitation | int(11)      | YES  |     | NULL    |       |

    | maxtemp       | int(11)      | YES  |     | NULL    |       |

    | mintemp       | int(11)      | YES  |     | NULL    |       |

    +---------------+--------------+------+-----+---------+-------+


    Use Sqoop to export the weather directory in HDFS to the weather table in MySQL on port 3306 on the namenode machine. The username for MySQL is root and the password is hadoop.


 create table 

 sqoop export \

 --connection jdbc:mysql//namenode/flightinfo \

 --username horton \

 --password hadoop \

 --table weather \

 --export-dir '/user/hortonworks/weather/' \

 --input-fields-terminated-by ',' ;



Task 1:

hadoop fs -mkdir flightdelays

hadoop fs -put ~/datasets/flightdelays/*.csv flightdelays/



Task 2:

a = load 'flightdelays' using PigStorage(',');

b = filter a by (chararray) $4 != 'NA' or (chararray) $11 != 'NA';

c = foreach b generate $0 as year:int, $1 as month:int, $2 as dayofmonth:int, $4 as deptime:int, 

$8 as uniquecarrier:chararray, $9 as flightnum:chararray, $14 as arrdelay:int, 

$16 as origin:chararray, $17 as dest:chararray;

store c into '/user/horton/flightdelays_clean' using PigStorage(',');



Task 3:

Step 1.

a = load 'flightdelays_clean';

b = group a all;

c = foreach b generate COUNT(a);

store c into 'cleaned_total';


Step 2.

a = load 'flightdelays_clean' using PigStorage(',');

b = filter a by (chararray) $8 == 'DEN';

c = group b all;

d = foreach c generate COUNT(b);

store d into '/user/horton/denver_total';


Step 3.

a = load 'flightdelays_clean' using PigStorage(',');

b = filter a by (chararray) $8 == 'DEN' and (int) $6 >= 60;

c = group b all;

d = foreach c generate COUNT(b);

store d into '/user/horton/denver_late';



Task 4:

create external table flightdelays (

year int, 

month int, 

dayofmonth int, 

deptime int, 

uniquecarrier string, 

flightnum int, 

arrdelay int, 

origin string, 

dest string) 

row format delimited 

fields terminated by ',' 

location '/user/horton/flightdelays_clean';



Task 5:

a = load 'flightdelays' using org.apache.hive.hcatalog.pig.HCatLoader();

b = filter a by arrdelay > 0;

c = order b by arrdelay desc parallel 3;

store c into '/user/horton/flightdelays_nonzero' using PigStorage(',');


Run the script with the following command:

$ pig -x tez -useHCatalog flightdelays_nonzero.pig


The output should consist of 13,265 records stored in three files:

-rw-r--r--   3 horton horton     123265   flightdelays_nonzero/part-v003-o000-r-00000

-rw-r--r--   3 horton horton     179581   flightdelays_nonzero/part-v003-o000-r-00001

-rw-r--r--   3 horton horton     123013   flightdelays_nonzero/part-v003-o000-r-00002



Task 6:

Step 1.

select avg(arrdelay) from flightdelays where dest = 'DEN';

Result is 7.26 minutes


Step 2.

select avg(arrdelay) from flightdelays where origin = 'LAX' and dest = 'SFO';

Result is 62.5 minutes


Step 3.

        Determine which dest airport had the highest average arrdelay


from flightdelays select dest, avg(arrdelay) as delay group by dest order by delay asc;


NOTE: There are many ways to find the highest arrdelay, but the answer is "SFO" with a value of about 55 minutes



Task 7:

create table sfo_weather_text (station_name string, year int, month int, dayofmonth int, precipitation int, temperature_max int, temperature_min int) row format delimited fields terminated by ',';

load data local inpath '/home/horton/datasets/flightdelays/sfo_weather.csv' into table sfo_weather_text;

create table sfo_weather stored as Orc as select * from sfo_weather_text;



Task 8:

set hive.execution.engine=tez;

create table flights_weather as select f.*, w.temperature_max, w.temperature_min from flightdelays as f  join sfo_weather as w on w.year = f.year and  w.month = f.month and  w.dayofmonth = f.dayofmonth where f.origin = "SFO" or f.dest = "SFO" ;



Task 9:

create table weather_partitioned (station_name string, dayofmonth int, precipipation int, temperature_max int, temperature_min int) partitioned by (year int, month int) stored as orc;

insert into table weather_partitioned partition(year=2008,month=1) select station_name, dayofmonth, precipitation, temperature_max, temperature_min from sfo_weather where year = 2008 and month = 1;


Task 10:

sqoop export --connect jdbc:mysql://namenode/flightinfo --table weather  --export-dir /user/horton/weather --input-fields-terminated-by ',' --username root --password hadoop

flume start
/usr/hdp/2.2.0.0-2041/flume$ bin/flume-ng agent -n agent -c conf -f /conf/conf.empty/flume-conf.properrties.template